# Configuración del Pipeline de GitHub Archive
# ----------------------------------------------

# General
project_name: "HackaDataFusion"
environment: "development"  # development, staging, production

# Configuración de descarga
download:
  # Rango de fechas (por defecto: ayer hasta hoy)
  start_date: "2023-05-01"  # Formato: YYYY-MM-DD
  end_date: "2023-05-03"    # Formato: YYYY-MM-DD
  
  # Configuración de descarga paralela
  max_workers: 5
  retry_attempts: 3
  retry_delay: 5  # segundos
  
  # Filtros adicionales (para implementación futura)
  event_types: []  # Vacío = todos los eventos

# Configuración de S3
s3_upload:
  enabled: true
  bucket: "hackadatafusion-bucket"  # Sustitúyelo por tu bucket real
  prefix: "github-archive"
  region: "us-east-1"
  max_workers: 5
  profile: null  # Si es nulo, usa las credenciales de AWS_ACCESS_KEY_ID y AWS_SECRET_ACCESS_KEY

# Validación de datos
data_quality:
  enabled: true
  # Umbrales para validación
  min_file_size_bytes: 1000
  max_error_rate: 0.05  # Tasa máxima de archivos erróneos tolerada
  schema_validation: true
  
  # Reglas específicas (para implementación futura)
  custom_rules: []

# Catálogo de datos
data_catalog:
  enabled: false  # Habilitar cuando se implemente
  catalog_type: "local"  # local, glue, datahub, etc.
  update_on_success: true
  
  # Configuraciones específicas para diferentes catálogos
  glue:
    database: "github_analytics"
    update_partitions: true
  
  datahub:
    url: "http://localhost:8080"
    token: ""

# Notificaciones
notifications:
  enabled: false
  type: "slack"  # slack, email, etc.
  
  # Solo enviar en estos casos
  on_success: true
  on_error: true
  
  # Configuración de Slack (opcional)
  slack_webhook_url: ""
  
  # Configuración de email (opcional)
  email_server: "smtp.example.com"
  email_port: 587
  email_use_tls: true
  email_username: ""
  email_password: ""
  email_sender: "pipeline@example.com"
  email_recipients: []

# Metadatos y seguimiento
metadata:
  tracking_enabled: true
  store_location: "metadata/"
  retention_days: 30  # Días para retener metadatos de ejecuciones antiguas

# Configuración de logs
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file_rotation_size_mb: 10
  file_backup_count: 5

# Opciones de rendimiento
performance:
  file_chunk_size_mb: 8  # Tamaño de chunks para subidas multipart
  batch_size: 100  # Número de archivos a procesar en cada lote