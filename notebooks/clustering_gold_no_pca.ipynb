{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fe3a46",
   "metadata": {},
   "source": [
    "# ðŸ“Š Clustering sobre mÃ©tricas agregadas de repositorios (Gold Layer)\n",
    "\n",
    "Este notebook evalÃºa distintos algoritmos de clustering sobre mÃ©tricas ya procesadas de repositorios, sin aplicar reducciÃ³n de dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7fabb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bad955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ruta del archivo generado por gold.py\n",
    "gold_path = 'data/gold/repo_metrics/repo_activity_metrics.parquet'\n",
    "\n",
    "try:\n",
    "    df = pd.read_parquet(gold_path)\n",
    "    print(f\"âœ” Datos cargados con Ã©xito: {df.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error al cargar el archivo: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef72391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solo columnas numÃ©ricas, excluyendo identificadores\n",
    "df_numeric = df.select_dtypes(include=[np.number])\n",
    "df_numeric = df_numeric.drop(columns=['repo_id'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_clustering(X, labels):\n",
    "    results = {}\n",
    "    if len(set(labels)) > 1 and -1 not in set(labels):\n",
    "        results['silhouette'] = silhouette_score(X, labels)\n",
    "        results['calinski'] = calinski_harabasz_score(X, labels)\n",
    "        results['davies'] = davies_bouldin_score(X, labels)\n",
    "    else:\n",
    "        results['silhouette'] = -1\n",
    "        results['calinski'] = -1\n",
    "        results['davies'] = np.inf\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd00ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca2fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "\n",
    "# KMeans\n",
    "for k in range(2, 11):\n",
    "    model = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = model.fit_predict(X_scaled)\n",
    "    scores = evaluate_clustering(X_scaled, labels)\n",
    "    results.append({'model': 'KMeans', 'params': {'k': k}, **scores})\n",
    "\n",
    "# DBSCAN\n",
    "for eps in [0.5, 1.0, 1.5]:\n",
    "    for min_samples in [3, 5, 10]:\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "        scores = evaluate_clustering(X_scaled, labels)\n",
    "        results.append({'model': 'DBSCAN', 'params': {'eps': eps, 'min_samples': min_samples}, **scores})\n",
    "\n",
    "# Agglomerative Clustering\n",
    "for k in range(2, 11):\n",
    "    model = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = model.fit_predict(X_scaled)\n",
    "    scores = evaluate_clustering(X_scaled, labels)\n",
    "    results.append({'model': 'Agglomerative', 'params': {'k': k}, **scores})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128fe854",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='silhouette', ascending=False)\n",
    "results_df.head(10)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
